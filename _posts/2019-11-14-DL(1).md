---
title: "딥러닝(1) - 개요, 퍼셉트론"
categories:
  - Deep Learning
tags:
  - Perceptron
  - Deep Learning
comments: true
---
* 목차
{:toc}

# 1. AI, DL, ML ... Neural Network
## 인공지능(AI)의 역사
- 1943년: 최초 신경망 모델 제안
- 1950년: 앨런 튜링이 튜링 테스트 제안
- (첫 번째 AI 붐) 1950년대 후반 ~ 1970년대 전반
	- 1956년 : 미국 다트머스의 학회에서 AI 이란 용어 처음 사용 
	- 1957년 : 코넬 항공 연구소 프랑크 로젠이 Perceptron 고안
- (두 번째 AI 붐) 1980년대: 주로 컴퓨터가 자율적으로 배우고 결정하는 종류가 아닌 인간으로부터 지식을 얻음 
- 1990년대: 세계 체스 챔피언을 물리 친 AI
- (세 번째 AI 붐) 2010년대: 딥 러닝 (머신 러닝)이라는 새로운 기술이 각광
<center><img src="https://mioscode.github.io/assets/images/history.jpg" width="80%"></center>

## Deep Learning (Machine Learning)
- 딥 러닝은 최신 인공 신경망(Artificial neural network) 기술로 인간 두뇌의 메커니즘을 시뮬레이션
- 인간의 뇌는 정보를 전달하기 위해 뉴런(neuron)과 뉴런들을 연결하는 시냅스(synapse)로 구성
- 인공 신경망(또는 단순히 신경망이라고 함)은 이러한 뉴런과 시냅스를 모델링
- 이 모델은 두 번째 AI 붐 중에 적극적으로 연구되었지만 레이어 매우 작았고, 이후 기술 발전으로 인해 대규모 네트워크로 확장되어 컴퓨터가 심층 학습을 실행할 수 있게됨
<center><img src="https://mioscode.github.io/assets/images/neural_network.jpg" width="80%"></center>

# 2. Perceptron

## 개념
- 1957년에 코넬 항공 연구소(Cornell Aeronautical Lab)의 프랑크 로젠블라트(Frank Rosenblatt)에 의해 고안
- 1957년, Rosenblatt이 뉴런(neuron)을 모방 하여 고안
- 뉴런: 특정 자극(impulse) 이 있다면 그 자극이 어느 역치(threshold) 이상이여야 반응(activation)
    - 활동전위(action potential)가 축삭(axon)을 따라 내려가면서 세포막 안팎의 극성이 변화 -> 다른 뉴런에서 온 신호에 반응하여 막이 역치(threshold) 전위에 도달하면 Na+ 및 K+ 개폐 이온 채널이 여닫힘 -> 활동전위가 시작될 때 Na+ 채널이 열리고 Na+가 축삭 안으로 들어와 탈분극(Depolarization), 재분극은 K+ 채널이 열리고 K+가 축삭 밖으로 나갈 때 일어남 -> 채널의 개폐로 세포 안팎 극성이 변화 -> 신경 자극은 한 방향으로만 이동하여 축삭 말단(axon ending)에서 다른 뉴런으로 신호를 전달
	<center><img src="https://mioscode.github.io/assets/images/action_potential.gif" width="80%"></center>
    <center><img src="https://mioscode.github.io/assets/images/action_potential.png" width="80%"></center>
    	(출처: 위키)
- 퍼셉트론: 입력(input) 신호의 총합이 정해진 임계값($\theta$) 넘었을 때 $1$을 출력(output), 넘지 못하면 $0$ 또는 $-1$ 출력

각 입력신호에 고유한 weight 부여되며 기계학습은 이 weight(입력을 조절하니 매개변수로도 볼 수 있음)의 값을 정하는 작업
![](/Users/somi.han/Documents/Deep%20Learning/Perceptron_3.png)

## 학습방법
1. 인간이 임의로 weight 설정하고 데이터 입력
2. 머신이 학습 데이터를 퍼셉트론 모형에 입력하며 분류가 잘못됐을 때 weight를 개선해 나간다
![](/Users/somi.han/Documents/Deep%20Learning/machine-learning-with-applications-in-categorization-popularity-and-sequence-labeling-75-638.jpg)

## 선형분류
학습이 반복될수록 선 기울기 달라지고 weight 조정됨
![](/Users/somi.han/Documents/Deep%20Learning/500px-Perceptron_example.svg.png)

## 퍼셉트론 한계점

### XOR

x1과 x2 중 어느 한쪽이 1일 때만 1을 출력

|x1|x2|y|
|:-:|:-:|:-:|
|0|0|0|
|1|0|1|
|0|1|1|
|1|1|0|

선형 분류 불가능
![](/Users/somi.han/Documents/Deep%20Learning/xor2.gif)

## 다층 퍼셉트론을 통한 한계 극복

![](/Users/somi.han/Documents/Deep%20Learning/Perceptron_XOR.jpg)

# Neural Network

## 개념
Input(입력층), Hidden(은닉층), Output(출력층)
![](/Users/somi.han/Documents/Deep%20Learning/300px-Colored_neural_network.svg.png)

## Activation Function

출력 값 결정하게 하는 역할, 변환기
AND, OR, NAND, XOR Gate 다음 단계

### 계단 함수 (Step Function)

![](/Users/somi.han/Documents/Deep%20Learning/download.png)

### 시그모이드 함수 (Sigmoid Function)

극단적인 계단 함수 대체하여 사용

![](/Users/somi.han/Documents/Deep%20Learning/sigmoid.png)

# 오차 역전파 (Error Backpropagatin)

* 순전파(Forwardpropagation) : 순차적으로 Input -> Output 가중치를 업데이트하면서 활성화 함수를 통해서 결과값을 가져오는 것

* 역전파 : 결과 값을 통해서 다시 역으로 input 방향으로 오차를 다시 보내며 가중치를 재업데이트 하는 것이다. 물론 결과에 영향을 많이 미친 노드(뉴런)에 더 많은 오차를 돌려줄 것이다.

![](/Users/somi.han/Documents/Deep%20Learning/H1KsG.png)

* 한번 역전파를 실행하는 것을 1 epoch 주기라고함
* epoch를 늘릴 수록 가중치가 계속 업데이트(학습)되면서 점점 오차가 줄어나가는 방법
 
![](/Users/somi.han/Documents/Deep%20Learning/997C7A3359EEF5CA1F.png)


## Reference
https://journal.jp.fujitsu.com/en/2016/02/09/01/
https://sacko.tistory.com/10?category=632408
https://www.neuraldesigner.com/blog/perceptron-the-main-component-of-neural-networks